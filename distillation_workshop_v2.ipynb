{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h9dET-pU2Kh"
      },
      "source": [
        "# ðŸŽ“ MenaML: Distillation Workshop\n",
        "\n",
        "**Author:** [Your Name]  \n",
        "**Date:** 2026\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this workshop, you will:\n",
        "1. Understand the core concepts behind Distillation\n",
        "2. Implement a Teacher-Student training framework from scratch\n",
        "3. Compare different training strategies: baseline, hard labels, soft labels\n",
        "4. Analyze the trade-offs between model size and performance\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sV45b9CU2Kh"
      },
      "source": [
        "## 1. Introduction: What is Knowledge Distillation?\n",
        "\n",
        "**Knowledge Distillation** is a technique whereby a student neural network learns from another, usually already pre-trained neural network. KD can be used for compressing the model, in which case the student is smaller. It can also be used for improving model performance, where the student is the same or even larger than the teacher. In either case, the the student is trained to mimic the behavior of one or more teacher models.\n",
        "\n",
        "### Why do we need it?\n",
        "\n",
        "- **Deployment constraints**: Large models are expensive to run on edge devices, mobile phones, or in real-time applications\n",
        "- **Inference speed**: Smaller models are faster\n",
        "- **Cost reduction**: Less compute = less money and energy\n",
        "- **Increased performance**: Sometimes KD is used to increase the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SJUO0XAl0Kx"
      },
      "source": [
        "## Why Does KD work?\n",
        "\n",
        "### The \"Dark Knowledge\"\n",
        "\n",
        "In their seminal 2015 paper ([Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531)), Hinton et al. observed that the **soft probability outputs** of a teacher model contain more information than hard labels.\n",
        "\n",
        "**Example**: For a cat image, hard label says `[0, 0, 1, 0, ...]` (just \"cat\")  \n",
        "But soft labels might say `[0.01, 0.05, 0.85, 0.09, ...]` revealing that the image also looks a bit like a dog or tiger!\n",
        "\n",
        "This extra information about class relationships is the \"dark knowledge\" that helps the student learn better.\n",
        "\n",
        "![Distillation Diagram](https://intellabs.github.io/distiller/imgs/knowledge_distillation.png)\n",
        "\n",
        "\n",
        "### Reweighing Training Examples\n",
        "\n",
        "Even if we ignore probabilities for all classes other than the true class, we get an effect where some examples are weighted higher than other ones according to what probability the teacher assigns to the true class. [Understanding and Improving Knowledge Distillation](https://arxiv.org/pdf/2002.03532) showed that this *importance weighing* is an important component of how KD works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6FpaMfYU2Ki"
      },
      "source": [
        "## 2. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akmnW8gjU2Ki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRWjNHReU2Ki"
      },
      "source": [
        "## 3. Dataset: CIFAR-10\n",
        "\n",
        "We'll use CIFAR-10: 60,000 32x32 color images in 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1MXgRc-U2Ki"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization\n",
        "cifar_mean = np.asarray([0.4914, 0.4822, 0.4465])\n",
        "cifar_std = np.asarray([0.2023, 0.1994, 0.2010])\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
        "])\n",
        "\n",
        "# Download and load datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=0)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(f\"Training samples: {len(trainset)}\")\n",
        "print(f\"Test samples: {len(testset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oySRHEcU2Ki"
      },
      "outputs": [],
      "source": [
        "# Visualize some samples\n",
        "def imshow(ax, img):\n",
        "    img = img * cifar_std[:, None, None] + cifar_mean[:, None, None]\n",
        "    img = np.clip(img, 0., 1.)\n",
        "    npimg = img.numpy()\n",
        "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n",
        "for i in range(8):\n",
        "    # axes[i].imshow(np.transpose((images[i] / 2 + 0.5).numpy(), (1, 2, 0)))\n",
        "    imshow(axes[i], images[i])\n",
        "    axes[i].set_title(classes[labels[i]])\n",
        "    axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buk8aA4EU2Ki"
      },
      "source": [
        "## 4. Model Definitions\n",
        "\n",
        "### Teacher Model: ResNet-18 (11M parameters)\n",
        "### Student Model: Small CNN (< 1M parameters)\n",
        "\n",
        "The goal is to transfer knowledge from the large teacher to the tiny student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYja44irU2Ki"
      },
      "outputs": [],
      "source": [
        "class TeacherCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Teacher: ResNet-18 adapted for CIFAR-10 (32x32 images)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, weights='IMAGENET1K_V1'):\n",
        "        super(TeacherCNN, self).__init__()\n",
        "        self.model = models.resnet18(weights=weights)\n",
        "        # Modify first conv layer for 32x32 images (no aggressive downsampling)\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.model.maxpool = nn.Identity()  # Remove maxpool for small images\n",
        "        self.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "        self.model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x, output_features=False):\n",
        "        x = self.model(x)\n",
        "        logits = self.fc(x)\n",
        "        if output_features:\n",
        "            return logits, x\n",
        "        return logits\n",
        "\n",
        "def get_teacher_model(num_classes=10, weights=None):\n",
        "    \"\"\"\n",
        "    Teacher: ResNet-18 adapted for CIFAR-10 (32x32 images)\n",
        "    \"\"\"\n",
        "    model = TeacherCNN(num_classes=num_classes, weights=weights)\n",
        "    return model\n",
        "\n",
        "\n",
        "class StudentCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Student: A small CNN with ~100K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, output_dim=256):\n",
        "        super(StudentCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 32 -> 16\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 16 -> 8\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 8 -> 4\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 4 -> 2\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 2 -> 1\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512 * 1 * 1, output_dim),  # 512 to match teacher\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(output_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, output_features=False):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        logits = self.fc2(x)\n",
        "        if output_features:\n",
        "            return logits, x\n",
        "        return logits\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WctECz5RJ5MS"
      },
      "source": [
        "### Activity 1. Instantiate the Teacher and Student models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6iucLqgLeRX"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "teacher = None\n",
        "student = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWsLk8bmJ1wL"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh55SIbzU2Ki"
      },
      "outputs": [],
      "source": [
        "# Create models and compare sizes\n",
        "teacher = get_teacher_model(weights='IMAGENET1K_V1').to(device)\n",
        "student = StudentCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt4EGqGPl0Kz"
      },
      "source": [
        "### Compare Student and Teacher Model Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHXbLK96l0Kz"
      },
      "outputs": [],
      "source": [
        "teacher_params = count_parameters(teacher)\n",
        "student_params = count_parameters(student)\n",
        "\n",
        "print(f\"Teacher (ResNet-18) parameters: {teacher_params:,}\")\n",
        "print(f\"Student (Small CNN) parameters: {student_params:,}\")\n",
        "print(f\"\\nCompression ratio: {teacher_params / student_params:.1f}x smaller\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUd59_i3U2Kj"
      },
      "source": [
        "## 5. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu1GAzEyU2Kj"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch (standard training)\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(trainloader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(trainloader), 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, device):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return 100. * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fi3_7I1U2Kj"
      },
      "source": [
        "## 6. Train the Teacher Model\n",
        "\n",
        "First, we need a well-trained teacher. In practice, you might use a pre-trained model, but we'll train one from scratch for educational purposes.\n",
        "\n",
        "**Note**: To save time, we'll provide the checkpoint, but you can check the training code and train your own teacher later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ScyNaA0uWV"
      },
      "source": [
        "### 6.1. Download the Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzQd6zjx2QEk"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1Ko41G-TVerBr1tY0cSr4m1h1s9PvHRXw\n",
        "\n",
        "# TODO: use these weights in the teacher!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXEguB550pey"
      },
      "source": [
        "### 6.2. Train the Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBLrBq-qU2Kj"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "TEACHER_EPOCHS = 15  # Increase for better teacher (50+ recommended)\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "teacher = get_teacher_model(weights='IMAGENET1K_V1').to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(teacher.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TEACHER_EPOCHS)\n",
        "\n",
        "print(\"Training Teacher Model (ResNet-18)...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "teacher_history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(TEACHER_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    train_loss, train_acc = train_epoch(teacher, trainloader, criterion, optimizer, device)\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(teacher, testloader, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    teacher_history['train_loss'].append(train_loss)\n",
        "    teacher_history['train_acc'].append(train_acc)\n",
        "    teacher_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{TEACHER_EPOCHS} | Loss: {train_loss:.4f} | \"\n",
        "        f\"Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | Epoch time \"\n",
        "        f\"{train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        # Save teacher model\n",
        "        torch.save(teacher.state_dict(), 'teacher_resnet18.pth')\n",
        "\n",
        "print(f\"\\nâœ… Teacher final test accuracy: {best_test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-smlZ6nEU2Kj"
      },
      "source": [
        "## 7. The Core: Knowledge Distillation Loss\n",
        "\n",
        "### The Distillation Loss Function\n",
        "The key innovation is combining two losses:\n",
        "$$L_{total} = \\alpha \\cdot L_{hard} + (1 - \\alpha) \\cdot L_{soft}$$\n",
        "Where:\n",
        "- $L_{hard}$ = Cross-entropy with true labels (standard supervised loss)\n",
        "- $L_{soft}$ = KL divergence between teacher and student soft predictions\n",
        "- $\\alpha$ = Weight balancing the two losses (typically 0.1-0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G2wB-ocL6uH"
      },
      "source": [
        "### Activity 2. Implement the `DistillationLoss` class in accordance with the previously provided description.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfucajXBMgCT"
      },
      "outputs": [],
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss\n",
        "\n",
        "    Combines:\n",
        "    1. Hard label loss: classification cross entropy.\n",
        "    2. Soft label loss: KL divergence between the student and teacher logits.\n",
        "\n",
        "    The T^2 factor compensates for the gradient magnitude reduction when using temperature.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.3):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # TODO\n",
        "        return total_loss, hard_loss, soft_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJR7uwBMa7n"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaju1hJFU2Kj"
      },
      "outputs": [],
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss\n",
        "\n",
        "    Combines:\n",
        "    1. Hard label loss: CrossEntropy(student_output, true_labels)\n",
        "    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n",
        "\n",
        "    The T^2 factor compensates for the gradient magnitude reduction when using temperature.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Hard label loss (standard cross-entropy)\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Soft label loss (KL divergence)\n",
        "        # Student: log-softmax\n",
        "        student_logits = F.log_softmax(student_logits, dim=1)\n",
        "        # Teacher: softmax (target distribution)\n",
        "        teacher_probs = F.softmax(teacher_logits, dim=1)\n",
        "\n",
        "        # KL divergence * T^2 (to match gradient magnitude)\n",
        "        soft_loss = self.kl_loss(student_logits, teacher_probs)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "        return total_loss, hard_loss, soft_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDlTPBQuU2Kj"
      },
      "source": [
        "## 8. Train Student with Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRMjsDB2OXMv"
      },
      "source": [
        "### Activity 3. Complete the knowledge distillation training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7vdqMw9O1M2"
      },
      "outputs": [],
      "source": [
        "def train_with_distillation(student, teacher, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Train student using knowledge distillation from teacher.\n",
        "    Teacher is frozen (no gradients).\n",
        "    \"\"\"\n",
        "    student.train()\n",
        "    # TODO: Teacher is always in eval mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_hard_loss = 0.0\n",
        "    running_soft_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(trainloader, desc=\"Distilling\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # TODO: Get teacher predictions (no gradient needed)\n",
        "\n",
        "        # Forward pass for student\n",
        "        optimizer.zero_grad()\n",
        "        student_logits = student(inputs)\n",
        "\n",
        "        # TODO: Compute distillation loss\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        running_hard_loss += hard_loss.item()\n",
        "        running_soft_loss += soft_loss.item()\n",
        "        _, predicted = student_logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    n = len(trainloader)\n",
        "    return running_loss/n, running_hard_loss/n, running_soft_loss/n, 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nEAzbSmO2KR"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecFADU5sU2Kj"
      },
      "outputs": [],
      "source": [
        "def train_with_distillation(student, teacher, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Train student using knowledge distillation from teacher.\n",
        "    Teacher is frozen (no gradients).\n",
        "    \"\"\"\n",
        "    student.train()\n",
        "    teacher.eval()  # Teacher is always in eval mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_hard_loss = 0.0\n",
        "    running_soft_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(trainloader, desc=\"Distilling\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Get teacher predictions (no gradient needed)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(inputs)\n",
        "\n",
        "        # Forward pass for student\n",
        "        optimizer.zero_grad()\n",
        "        student_logits = student(inputs)\n",
        "\n",
        "        # Compute distillation loss\n",
        "        loss, hard_loss, soft_loss = criterion(student_logits, teacher_logits, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        running_hard_loss += hard_loss.item()\n",
        "        running_soft_loss += soft_loss.item()\n",
        "        _, predicted = student_logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    n = len(trainloader)\n",
        "    return running_loss/n, running_hard_loss/n, running_soft_loss/n, 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkpsqpVQOpJJ"
      },
      "source": [
        "### Training with distillation (Using big network as teacher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDSeHYWTU2Kj"
      },
      "outputs": [],
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 30\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "\n",
        "# Load trained teacher\n",
        "teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n",
        "teacher.eval()\n",
        "\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "\n",
        "# Setup training\n",
        "distill_criterion = DistillationLoss(alpha=ALPHA)\n",
        "optimizer = optim.Adam(student_distilled.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "print(f\"Training Student with Knowledge Distillation\")\n",
        "print(f\"Alpha: {ALPHA}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    total_loss, hard_loss, soft_loss, train_acc = train_with_distillation(\n",
        "        student_distilled, teacher, trainloader, distill_criterion, optimizer, device\n",
        "    )\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_distilled, testloader, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} \"\n",
        "        f\"(H:{hard_loss:.3f} S:{soft_loss:.3f}) | \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_distilled.state_dict(), 'student_distilled_v1.pth')\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {best_test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF7uzQ5WU2Kj"
      },
      "source": [
        "## 9. Baseline: Train Student WITHOUT Distillation\n",
        "\n",
        "To appreciate the value of distillation, let's train the same student architecture with only hard labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ansQrnAHU2Kj"
      },
      "outputs": [],
      "source": [
        "# Train student from scratch (no teacher)\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "criterion_baseline = nn.CrossEntropyLoss()\n",
        "optimizer_baseline = optim.Adam(student_baseline.parameters(), lr=0.01)\n",
        "scheduler_baseline = optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "print(\"Training Student Baseline (NO distillation)...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "baseline_history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "best_test_acc = 0.0\n",
        "\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    train_loss, train_acc = train_epoch(student_baseline, trainloader, criterion_baseline, optimizer_baseline, device)\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_baseline, testloader, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    baseline_history['train_loss'].append(train_loss)\n",
        "    baseline_history['train_acc'].append(train_acc)\n",
        "    baseline_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {train_loss:.4f} \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_baseline.state_dict(), 'student_baseline.pth')\n",
        "print(f\"\\nâœ… Baseline Student final test accuracy: {best_test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ2EjM3zU2Kj"
      },
      "source": [
        "## 10. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHzjg9PU2Kj"
      },
      "outputs": [],
      "source": [
        "# Final comparison\n",
        "teacher_acc = evaluate(teacher, testloader, device)\n",
        "distilled_acc = evaluate(student_distilled, testloader, device)\n",
        "baseline_acc = evaluate(student_baseline, testloader, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<25} {'Parameters':<15} {'Test Accuracy':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Teacher (ResNet-18)':<25} {teacher_params:>12,} {teacher_acc:>12.2f}%\")\n",
        "print(f\"{'Student (Baseline)':<25} {student_params:>12,} {baseline_acc:>12.2f}%\")\n",
        "print(f\"{'Student (Distilled)':<25} {student_params:>12,} {distilled_acc:>12.2f}%\")\n",
        "print(\"-\"*60)\n",
        "print(f\"\\nðŸ“ˆ Distillation improvement: +{distilled_acc - baseline_acc:.2f}%\")\n",
        "print(f\"ðŸ—œï¸  Compression ratio: {teacher_params / student_params:.1f}x fewer parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2UF_dpoU2Kj"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Test accuracy comparison\n",
        "axes[0].plot(baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n",
        "axes[0].plot(distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n",
        "axes[0].axhline(y=teacher_acc, color='r', linestyle=':', label=f'Teacher ({teacher_acc:.1f}%)')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Test Accuracy (%)')\n",
        "axes[0].set_title('Test Accuracy Comparison')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss breakdown for distilled student\n",
        "axes[1].plot(distilled_history['hard_loss'], label='Hard Loss (CE)', alpha=0.8)\n",
        "axes[1].plot(distilled_history['soft_loss'], label='Soft Loss (KL)', alpha=0.8)\n",
        "axes[1].plot(distilled_history['total_loss'], label='Total Loss', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Distillation Loss Components')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q69bvOz2U2Kj"
      },
      "source": [
        "## 11. Inference Speed Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-MgdJaOl0K7"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(model, input_size=(1, 3, 32, 32), num_iterations=1000):\n",
        "    \"\"\"Benchmark inference speed\"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(100):\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # Benchmark\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(num_iterations):\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    return elapsed / num_iterations * 1000  # ms per inference\n",
        "\n",
        "teacher_time = benchmark_inference(teacher)\n",
        "student_time = benchmark_inference(student_distilled)\n",
        "\n",
        "print(\"\\nâš¡ Inference Speed (single sample)\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Teacher (ResNet-18): {teacher_time:.3f} ms\")\n",
        "print(f\"Student (Distilled): {student_time:.3f} ms\")\n",
        "print(f\"Speedup: {teacher_time / student_time:.2f}x faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdn6Q-13l0K7"
      },
      "source": [
        "## 12. Temperature Scaling\n",
        "We \"soften\" the probability distributions using temperature $T$:\n",
        "$$p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
        "- $T = 1$: Normal softmax\n",
        "- $T > 1$: Softer distribution (reveals more information about class relationships)\n",
        "- Typically $T \\in [3, 20]$\n",
        "\n",
        "Note that the temperature is applied to both the teacher and the student outputs.\n",
        "As the temperature T increases, the gradient of the loss gets rescaled by a factor of (1/T)^2. This has been derived in the original Hinton paper. As decreasing gradient value can affect optimization dynamics, we need to upscale the gradients by a factor of T^2. The easiest way to do it is just to multiply the loss by T^2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYVaMIVdl0K7"
      },
      "source": [
        "### ðŸ” Let's Visualize Temperature Effects\n",
        "\n",
        "Understanding how temperature affects the probability distribution is crucial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUBaKjTzl0K7"
      },
      "outputs": [],
      "source": [
        "# Demonstrate temperature effect\n",
        "def visualize_temperature_effect(logits, temperatures=[1, 2, 4, 8, 16]):\n",
        "    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n",
        "\n",
        "    for ax, T in zip(axes, temperatures):\n",
        "        probs = F.softmax(logits / T, dim=0).numpy()\n",
        "        ax.bar(range(len(probs)), probs, color='steelblue')\n",
        "        ax.set_title(f'T = {T}')\n",
        "        ax.set_xlabel('Class')\n",
        "        ax.set_ylabel('Probability')\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xticks(range(10))\n",
        "\n",
        "    plt.suptitle('Effect of Temperature on Softmax Distribution', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example logits (model is quite confident about class 3)\n",
        "example_logits = torch.tensor([1.0, 2.0, 1.5, 8.0, 0.5, 2.5, 1.0, 0.8, 1.2, 1.8])\n",
        "visualize_temperature_effect(example_logits)\n",
        "\n",
        "print(\"\\nðŸ“Š Observation:\")\n",
        "print(\"- T=1: Sharp distribution (almost one-hot)\")\n",
        "print(\"- T>1: Softer distribution revealing class relationships\")\n",
        "print(\"- Higher T = more 'dark knowledge' transferred\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuR09EPCl0K7"
      },
      "source": [
        "### Activity 4. Implement the `TemperedDistillationLoss` class and its training loop.\n",
        "It should be similar to `DistillationLoss` but with the temperature scaling applied to the teacher and student probabilities, and rescaled loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apJA40V1l0K7"
      },
      "outputs": [],
      "source": [
        "class TemperedDistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss\n",
        "\n",
        "    Combines:\n",
        "    1. Hard label loss: CrossEntropy(student_output, true_labels)\n",
        "    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n",
        "\n",
        "    The T^2 factor compensates for the gradient magnitude reduction when using temperature.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=4.0, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Hard label loss (standard cross-entropy)\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # TODO: student_soft and teacher_soft with temperature scaling.\n",
        "        student_soft = None\n",
        "        teacher_soft = None\n",
        "\n",
        "        # KL divergence * T^2 (to match gradient magnitude)\n",
        "        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "        return total_loss, hard_loss, soft_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 30\n",
        "TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "\n",
        "# Load trained teacher\n",
        "# TODO\n",
        "\n",
        "# Load Student\n",
        "# TODO\n",
        "\n",
        "# Setup training\n",
        "# TODO\n",
        "\n",
        "# Training loop\n",
        "print(f\"Training Student with Tempered Knowledge Distillation\")\n",
        "print(f\"Alpha: {ALPHA}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    total_loss, hard_loss, soft_loss, train_acc = train_with_distillation(\n",
        "        student_distilled, teacher, trainloader, distill_criterion, optimizer, device\n",
        "    )\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_distilled, testloader, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} \"\n",
        "        f\"(H:{hard_loss:.3f} S:{soft_loss:.3f}) | \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_distilled.state_dict(), 'student_distilled.pth')\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {best_test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "xNAjewNz5B3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ64JBPIl0K7"
      },
      "source": [
        "#### Solution Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbbTQ-tXl0K7"
      },
      "outputs": [],
      "source": [
        "class TemperedDistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss\n",
        "\n",
        "    Combines:\n",
        "    1. Hard label loss: CrossEntropy(student_output, true_labels)\n",
        "    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n",
        "\n",
        "    The T^2 factor compensates for the gradient magnitude reduction when using temperature.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=4.0, alpha=0.3):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Hard label loss (standard cross-entropy)\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Soft label loss (KL divergence with temperature)\n",
        "        # Student: log-softmax with temperature\n",
        "        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        # Teacher: softmax with temperature (target distribution)\n",
        "        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "\n",
        "        # KL divergence * T^2 (to match gradient magnitude)\n",
        "        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "        return total_loss, hard_loss, soft_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train with the TemperedDistillationLoss"
      ],
      "metadata": {
        "id": "d2O9UpOY3M1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 5\n",
        "TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "\n",
        "# Load trained teacher\n",
        "teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n",
        "teacher.eval()\n",
        "\n",
        "# Load Student\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "\n",
        "# Setup training\n",
        "distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n",
        "optimizer = optim.Adam(student_distilled.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "# Training loop\n",
        "print(f\"Training Student with Tempered Knowledge Distillation\")\n",
        "print(f\"Alpha: {ALPHA} Temperature: {TEMPERATURE}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    total_loss, hard_loss, soft_loss, train_acc = train_with_distillation(\n",
        "        student_distilled, teacher, trainloader, distill_criterion, optimizer, device\n",
        "    )\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_distilled, testloader, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} \"\n",
        "        f\"(H:{hard_loss:.3f} S:{soft_loss:.3f}) | \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_distilled.state_dict(), 'student_distilled.pth')\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {best_test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "a4HwHO2C3DT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Distillation can works on downstream tasks."
      ],
      "metadata": {
        "id": "-yvqeRdYvK07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grayscale + Noise (old camera simulation)\n",
        "class AddGaussianNoise:\n",
        "    def __init__(self, mean=0., std=0.1):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
        "        return torch.clamp(tensor + noise, 0., 1.)\n",
        "\n",
        "transform_train_grayscale_noisy = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0., std=0.02),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "transform_test_grayscale_noisy = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0., std=0.02),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "# Download and load datasets\n",
        "trainset_v2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train_grayscale_noisy)\n",
        "testset_v2 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test_grayscale_noisy)\n",
        "\n",
        "trainloader_v2 = DataLoader(trainset_v2, batch_size=128, shuffle=True, num_workers=0)\n",
        "testloader_v2 = DataLoader(testset_v2, batch_size=128, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "xc-hiORAvJ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 5. Plot some instances"
      ],
      "metadata": {
        "id": "UotP-OxBGGhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the original CIFAR-10 dataset, plot a few sample instances to assess the impact of the applied transformation."
      ],
      "metadata": {
        "id": "JQXNRzKpGNNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some random training images\n",
        "dataiter_v2 = iter(trainloader_v2)\n",
        "images, labels = next(dataiter_v2)\n",
        "\n",
        "# Show images\n",
        "# TODO"
      ],
      "metadata": {
        "id": "VKB9Na2FGmd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Activity 5."
      ],
      "metadata": {
        "id": "35JhdFNIGlzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some random training images\n",
        "dataiter_v2 = iter(trainloader_v2)\n",
        "images, labels = next(dataiter_v2)\n",
        "\n",
        "# Show images\n",
        "fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n",
        "for i in range(8):\n",
        "    # axes[i].imshow(np.transpose((images[i] / 2 + 0.5).numpy(), (1, 2, 0)))\n",
        "    imshow(axes[i], images[i])\n",
        "    axes[i].set_title(classes[labels[i]])\n",
        "    axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "atsYBOMD7weP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 6. Train the student without distillation on the new dataset."
      ],
      "metadata": {
        "id": "G_PtcsQ1Gyeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "MuXMrcLNG7SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Activity 6"
      ],
      "metadata": {
        "id": "7rtuTO4HHGkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train student from scratch (no teacher)\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "criterion_baseline = nn.CrossEntropyLoss()\n",
        "optimizer_baseline = optim.Adam(student_baseline.parameters(), lr=0.01)\n",
        "scheduler_baseline = optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "print(\"Training Student Baseline (NO distillation)...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "baseline_history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "best_test_acc = 0.0\n",
        "\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    train_loss, train_acc = train_epoch(student_baseline, trainloader_v2, criterion_baseline, optimizer_baseline, device)\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_baseline, testloader_v2, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    baseline_history['train_loss'].append(train_loss)\n",
        "    baseline_history['train_acc'].append(train_acc)\n",
        "    baseline_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {train_loss:.4f} \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_baseline.state_dict(), 'student_baseline.pth')\n",
        "print(f\"\\nâœ… Baseline Student final test accuracy: {best_test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "jgNQFu9GD5p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 7. Train the student network on the new dataset using the TemperedDistillationLoss class, and compare the results against those obtained from training the student model independently."
      ],
      "metadata": {
        "id": "Wjrjh8dzHKvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "ptMQr6ntJH0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Activity 7."
      ],
      "metadata": {
        "id": "T4jPnF0sJI7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 5\n",
        "TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "\n",
        "# Load trained teacher\n",
        "teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n",
        "teacher.eval()\n",
        "\n",
        "# Load Student\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "\n",
        "# Setup training\n",
        "distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n",
        "optimizer = optim.Adam(student_distilled.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "# Training loop\n",
        "print(f\"Training Student with Tempered Knowledge Distillation\")\n",
        "print(f\"Alpha: {ALPHA} Temperature: {TEMPERATURE}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    train_start = time.time()\n",
        "    total_loss, hard_loss, soft_loss, train_acc = train_with_distillation(\n",
        "        student_distilled, teacher, trainloader_v2, distill_criterion, optimizer, device\n",
        "    )\n",
        "    train_time = time.time() - train_start\n",
        "    eval_start = time.time()\n",
        "    test_acc = evaluate(student_distilled, testloader_v2, device)\n",
        "    eval_time = time.time() - eval_start\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} \"\n",
        "        f\"(H:{hard_loss:.3f} S:{soft_loss:.3f}) | \"\n",
        "        f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% |\"\n",
        "        f\" Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_distilled.state_dict(), 'student_distilled.pth')\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {best_test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "UrMmYWWY9_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6a2I0nIl0K7"
      },
      "source": [
        "## 14. Born Again Networks (Use the previous trained model (Student) as a Teacher)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUrkX-g8l0K8"
      },
      "outputs": [],
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 30\n",
        "TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "\n",
        "# Load trained teacher\n",
        "teacher_dim = teacher.fc.in_features\n",
        "teacher_st_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "teacher_st_baseline.load_state_dict(torch.load('student_baseline.pth'))\n",
        "teacher_st_baseline.eval()\n",
        "\n",
        "student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "\n",
        "# Setup training\n",
        "distill_criterion = DistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n",
        "optimizer = optim.Adam(student_distilled.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "print(f\"Training Student with Knowledge Distillation\")\n",
        "print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    total_loss, hard_loss, soft_loss, train_acc = train_with_distillation(\n",
        "        student_distilled, teacher_st_baseline, trainloader, distill_criterion, optimizer, device\n",
        "    )\n",
        "    test_acc = evaluate(student_distilled, testloader, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} (H:{hard_loss:.3f} S:{soft_loss:.3f}) | \"\n",
        "          f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(student_distilled.state_dict(), 'student_distilled_v2.pth')\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {best_test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BKAMBSPU2Kj"
      },
      "source": [
        "## 14. ðŸ§ª Experiment: Hyperparameter Sensitivity\n",
        "\n",
        "Try different values of **Temperature** and **Alpha** to see their effects!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-uv75XeU2Kj"
      },
      "outputs": [],
      "source": [
        "def quick_distillation_experiment(temperature, alpha, epochs=15):\n",
        "    \"\"\"Quick experiment with different hyperparameters\"\"\"\n",
        "    student = StudentCNN().to(device)\n",
        "    criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_with_distillation(student, teacher, trainloader, criterion, optimizer, device)\n",
        "\n",
        "    return evaluate(student, testloader, device)\n",
        "\n",
        "# Uncomment to run experiments (takes a few minutes)\n",
        "# print(\"Running hyperparameter experiments...\")\n",
        "\n",
        "# temperatures = [1, 2, 4, 8, 16]\n",
        "# alphas = [0.1, 0.3, 0.5, 0.7]\n",
        "\n",
        "# results = {}\n",
        "# for T in temperatures:\n",
        "#     for a in alphas:\n",
        "#         acc = quick_distillation_experiment(T, a)\n",
        "#         results[(T, a)] = acc\n",
        "#         print(f\"T={T}, Î±={a}: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbJ2ZtW2U2Kk"
      },
      "source": [
        "## 15. Types of Knowledge Distillation\n",
        "\n",
        "What we implemented is **Response-based Distillation**. There are other types:\n",
        "\n",
        "| Type | What's Transferred | Example |\n",
        "|------|-------------------|--------|\n",
        "| **Response-based** | Final output logits | What we did! |\n",
        "| **Feature-based** | Intermediate representations | FitNets, Attention Transfer |\n",
        "| **Relation-based** | Relationships between samples | Contrastive distillation |\n",
        "\n",
        "### Bonus: Feature-based Distillation (FitNets)\n",
        "\n",
        "The idea is to also match intermediate feature maps, not just outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMPhV21tU2Kk"
      },
      "outputs": [],
      "source": [
        "class FeatureDistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature-based distillation: match intermediate representations.\n",
        "    Requires a projection layer if dimensions don't match.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, student_dim, teacher_dim, temperature=4.0, alpha=0.3, beta=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta  # Weight for feature loss\n",
        "\n",
        "        # Projection layer if dimensions don't match\n",
        "        self.projector = nn.Linear(student_dim, teacher_dim) if student_dim != teacher_dim else nn.Identity()\n",
        "        self.projector = self.projector.to(device)\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n",
        "        # Standard distillation losses\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "        soft_loss = self.kl_loss(\n",
        "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        # Feature matching loss\n",
        "        student_proj = self.projector(student_features)\n",
        "        feature_loss = 1 - F.cosine_similarity(student_proj, teacher_features).mean()\n",
        "\n",
        "        # Combined loss\n",
        "        total = self.alpha * hard_loss + (1 - self.alpha - self.beta) * soft_loss + self.beta * 10 * feature_loss\n",
        "\n",
        "        return total, hard_loss, soft_loss, feature_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq63hq70etll"
      },
      "outputs": [],
      "source": [
        "def train_with_feature_distillation(student, teacher, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Train student using knowledge distillation from teacher.\n",
        "    Teacher is frozen (no gradients).\n",
        "    \"\"\"\n",
        "    student.train()\n",
        "    teacher.eval()  # Teacher is always in eval mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_hard_loss = 0.0\n",
        "    running_soft_loss = 0.0\n",
        "    running_feature_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(trainloader, desc=\"Distilling\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Get teacher predictions (no gradient needed)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits, teacher_features = teacher(inputs, output_features=True)\n",
        "\n",
        "        # Forward pass for student\n",
        "        optimizer.zero_grad()\n",
        "        student_logits, student_features = student(inputs, output_features=True)\n",
        "\n",
        "        # Compute distillation loss\n",
        "        loss, hard_loss, soft_loss, feature_loss = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        running_hard_loss += hard_loss.item()\n",
        "        running_soft_loss += soft_loss.item()\n",
        "        running_feature_loss += feature_loss.item()\n",
        "        _, predicted = student_logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    n = len(trainloader)\n",
        "    return running_loss/n, running_hard_loss/n, running_soft_loss/n, running_feature_loss/n, 100.*correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqGC_W1-aGoa"
      },
      "outputs": [],
      "source": [
        "# Distillation configuration\n",
        "STUDENT_EPOCHS = 30\n",
        "TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n",
        "ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n",
        "LR = 0.01\n",
        "BETA = 0.5\n",
        "\n",
        "# Load trained teacher\n",
        "teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n",
        "teacher.eval()\n",
        "\n",
        "# Create fresh student\n",
        "teacher_dim = teacher.fc.in_features\n",
        "student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n",
        "\n",
        "# Setup training\n",
        "student_dim = student_distilled.fc2[1].in_features\n",
        "teacher_dim = teacher.fc.in_features\n",
        "distill_criterion = FeatureDistillationLoss(device, student_dim, teacher_dim, temperature=TEMPERATURE, alpha=ALPHA, beta=BETA)\n",
        "parameters = list(student_distilled.parameters()) + list(distill_criterion.projector.parameters())\n",
        "optimizer = optim.Adam(parameters, lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n",
        "\n",
        "print(f\"Training Student with Knowledge Distillation\")\n",
        "print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}, Beta: {BETA}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'feature_loss': [],'train_acc': [], 'test_acc': []}\n",
        "\n",
        "for epoch in range(STUDENT_EPOCHS):\n",
        "    total_loss, hard_loss, soft_loss, feature_loss, train_acc = train_with_feature_distillation(\n",
        "        student_distilled, teacher, trainloader, distill_criterion, optimizer, device\n",
        "    )\n",
        "    test_acc = evaluate(student_distilled, testloader, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    distilled_history['total_loss'].append(total_loss)\n",
        "    distilled_history['hard_loss'].append(hard_loss)\n",
        "    distilled_history['soft_loss'].append(soft_loss)\n",
        "    distilled_history['feature_loss'].append(feature_loss)\n",
        "    distilled_history['train_acc'].append(train_acc)\n",
        "    distilled_history['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} (H:{hard_loss:.3f} S:{soft_loss:.3f} F:{feature_loss:.3f}) | \"\n",
        "          f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nâœ… Distilled Student final test accuracy: {test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtuOdh5rU2Kk"
      },
      "source": [
        "## 16. Key Takeaways\n",
        "\n",
        "### âœ… What We Learned\n",
        "\n",
        "1. **Knowledge Distillation** transfers \"dark knowledge\" from a large teacher to a small student\n",
        "\n",
        "2. **Soft labels** contain richer information than hard labels (class relationships)\n",
        "\n",
        "3. **Temperature** controls how soft the probability distribution is:\n",
        "   - Higher T = more information transfer, but potentially noisier\n",
        "   - Typical values: 2-20\n",
        "\n",
        "4. **Alpha** balances hard and soft losses:\n",
        "   - Lower Î± = more emphasis on mimicking teacher\n",
        "   - Higher Î± = more emphasis on ground truth\n",
        "\n",
        "5. **Results**: Distilled students typically outperform students trained from scratch by 1-5%\n",
        "\n",
        "### ðŸš€ Extensions to Explore\n",
        "\n",
        "- **Self-distillation**: Use the same architecture for teacher and student\n",
        "- **Online distillation**: Train teacher and student simultaneously\n",
        "- **Multi-teacher distillation**: Ensemble of teachers\n",
        "- **Task-specific distillation**: For NLP, use DistilBERT approach\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "1. Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n",
        "2. Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2015)\n",
        "3. Gou et al., \"Knowledge Distillation: A Survey\" (2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMl6-H3jU2Kk"
      },
      "source": [
        "## 15. ðŸ’ª Exercise for You!\n",
        "\n",
        "Try these modifications and see what happens:\n",
        "\n",
        "1. **Change the student architecture**: Make it deeper or wider\n",
        "2. **Try different temperatures**: Plot accuracy vs temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jbrT3-6U2Kk"
      },
      "outputs": [],
      "source": [
        "# Your experiments here!\n",
        "#\n",
        "# Example: Try a deeper student\n",
        "# class DeeperStudent(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # Your architecture here\n",
        "#         pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}