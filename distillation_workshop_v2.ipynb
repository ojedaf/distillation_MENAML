{"cells":[{"cell_type":"markdown","metadata":{"id":"-h9dET-pU2Kh"},"source":["# ðŸŽ“ MenaML: Distillation Workshop\n","\n","**Author:** [Your Name]  \n","**Date:** 2026\n","\n","---\n","\n","## Learning Objectives\n","\n","By the end of this workshop, you will:\n","1. Understand the core concepts behind Distillation\n","2. Implement a Teacher-Student training framework from scratch\n","3. Compare different training strategies: baseline, hard labels, soft labels\n","4. Analyze the trade-offs between model size and performance\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"9sV45b9CU2Kh"},"source":["## 1. Introduction: What is Knowledge Distillation?\n","\n","**Knowledge Distillation** is a technique whereby a student neural network learns from another, usually already pre-trained neural network. KD can be used for compressing the model, in which case the student is smaller. It can also be used for improving model performance, where the student is the same or even larger than the teacher. In either case, the the student is trained to mimic the behavior of one or more teacher models.\n","\n","### Why do we need it?\n","\n","- **Deployment constraints**: Large models are expensive to run on edge devices, mobile phones, or in real-time applications\n","- **Inference speed**: Smaller models are faster\n","- **Cost reduction**: Less compute = less money and energy\n","- **Increased performance**: Sometimes KD is used to increase the performance of our model."]},{"cell_type":"markdown","metadata":{"id":"mmk4A2LunBrH"},"source":["## Why Does KD work?\n","\n","### The \"Dark Knowledge\"\n","\n","In their seminal 2015 paper, [Hinton et al](https://arxiv.org/pdf/1503.02531). observed that the **soft probability outputs** of a teacher model contain more information than hard labels.\n","\n","**Example**: For a cat image, hard label says `[0, 0, 1, 0, ...]` (just \"cat\")  \n","But soft labels might say `[0.01, 0.05, 0.85, 0.09, ...]` revealing that the image also looks a bit like a dog or tiger!\n","\n","This extra information about class relationships is the \"dark knowledge\" that helps the student learn better.\n","\n","![Distillation Diagram](https://intellabs.github.io/distiller/imgs/knowledge_distillation.png)\n","\n","\n","### Reweighing Training Examples\n","\n","Even if we ignore probabilities for all classes other than the true class, we get an effect where some examples are weighted higher than other ones according to what probability the teacher assigns to the true class. [Tang et al](https://arxiv.org/pdf/2002.03532) showed that this *importance weighing* is an important component of how KD works."]},{"cell_type":"markdown","metadata":{"id":"n6FpaMfYU2Ki"},"source":["## 2. Setup and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akmnW8gjU2Ki"},"outputs":[],"source":["import collections\n","import os\n","import time\n","from typing import Dict, List, Optional, Tuple, Union\n","\n","# Third party\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","import tqdm.auto as tqdm\n","\n","# Check GPU availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"markdown","metadata":{"id":"nRWjNHReU2Ki"},"source":["## 3. Dataset: CIFAR-10\n","\n","We'll use CIFAR-10: 60,000 32x32 color images in 10 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1MXgRc-U2Ki"},"outputs":[],"source":["# Data augmentation and normalization\n","cifar_mean = np.asarray([0.4914, 0.4822, 0.4465])\n","cifar_std = np.asarray([0.2023, 0.1994, 0.2010])\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n","])\n","\n","# Download and load datasets\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","trainloader = DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=0,\n",")\n","testloader = DataLoader(\n","    testset, batch_size=128, shuffle=False, num_workers=0,\n",")\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","print(f\"Training samples: {len(trainset)}\")\n","print(f\"Test samples: {len(testset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oySRHEcU2Ki"},"outputs":[],"source":["# Visualize some samples\n","def imshow(ax, img: torch.Tensor) -> None:\n","    \"\"\"Helper function to un-normalize and display an image.\n","\n","    Args:\n","        ax: Matplotlib axes to plot on.\n","        img (torch.Tensor): Tensor image of shape (C, H, W).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    img = img * cifar_std[:, None, None] + cifar_mean[:, None, None]\n","    img = np.clip(img, 0., 1.)\n","    npimg = img.numpy()\n","    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","# Get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# Show images\n","fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n","for i in range(8):\n","    # axes[i].imshow(np.transpose((images[i] / 2 + 0.5).numpy(), (1, 2, 0)))\n","    imshow(axes[i], images[i])\n","    axes[i].set_title(classes[labels[i]])\n","    axes[i].axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Buk8aA4EU2Ki"},"source":["## 4. Model Definitions\n","\n","### Teacher Model: ResNet-18 (11M parameters)\n","### Student Model: Small CNN (< 1M parameters)\n","\n","The goal is to transfer knowledge from the large teacher to the tiny student."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYja44irU2Ki"},"outputs":[],"source":["class TeacherCNN(nn.Module):\n","    \"\"\"\n","    Teacher: ResNet-18 adapted for CIFAR-10 (32x32 images)\n","    \"\"\"\n","    def __init__(self, num_classes: int = 10, weights: Optional[str] = None):\n","        super(TeacherCNN, self).__init__()\n","        self.model = models.resnet18(weights=weights)\n","        # Modify first conv layer for 32x32 images (no aggressive downsampling)\n","        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.model.maxpool = nn.Identity()  # Remove maxpool for small images\n","        self.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","        self.model.fc = nn.Identity()\n","\n","    def forward(self, x, output_features=False):\n","        x = self.model(x)\n","        logits = self.fc(x)\n","        if output_features:\n","            return logits, x\n","        return logits\n","\n","def get_teacher_model(num_classes: int = 10, weights: Optional[str] = 'IMAGENET1K_V1'):\n","    \"\"\"\n","    Teacher: ResNet-18 adapted for CIFAR-10 (32x32 images)\n","    \"\"\"\n","    model = TeacherCNN(num_classes=num_classes, weights=weights)\n","    return model\n","\n","\n","class StudentCNN(nn.Module):\n","    \"\"\"\n","    Student: A small CNN with ~100K parameters\n","    \"\"\"\n","    def __init__(self, num_classes: int = 10, output_dim: int = 256):\n","        super(StudentCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            # Block 1\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 32 -> 16\n","\n","            # Block 2\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 16 -> 8\n","\n","            # Block 3\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 8 -> 4\n","\n","            # Block 4\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 4 -> 2\n","\n","            # Block 5\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 2 -> 1\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(512 * 1 * 1, output_dim),  # 512 to match teacher\n","            nn.ReLU(),\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(output_dim, num_classes),\n","        )\n","\n","    def forward(self, x, output_features=False):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        logits = self.fc2(x)\n","        if output_features:\n","            return logits, x\n","        return logits\n","\n","\n","def count_parameters(model):\n","    \"\"\"Count trainable parameters\"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"WctECz5RJ5MS"},"source":["### Activity 1. Instantiate the Teacher and Student models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6iucLqgLeRX"},"outputs":[],"source":["# TODO\n","teacher = None\n","student = None"]},{"cell_type":"markdown","metadata":{"id":"zWsLk8bmJ1wL"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh55SIbzU2Ki"},"outputs":[],"source":["# Create models and compare sizes\n","teacher = get_teacher_model().to(device)\n","student = StudentCNN().to(device)"]},{"cell_type":"markdown","metadata":{"id":"ewVy-h6dnBrI"},"source":["### Compare Student and Teacher Model Sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLn0g_JknBrI"},"outputs":[],"source":["teacher_params = count_parameters(teacher)\n","student_params = count_parameters(student)\n","\n","print(f\"Teacher (ResNet-18) parameters: {teacher_params:,}\")\n","print(f\"Student (Small CNN) parameters: {student_params:,}\")\n","print(f\"\\nCompression ratio: {teacher_params / student_params:.1f}x smaller\")"]},{"cell_type":"markdown","metadata":{"id":"MUd59_i3U2Kj"},"source":["## 5. Training Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gu1GAzEyU2Kj"},"outputs":[],"source":["def train_epoch(model, teacher, trainloader, criterion, optimizer, device):\n","    \"\"\"Trains the model using supervised training or distillation.\n","\n","    Distillation is used only when the teacher is present. In that case, the\n","    teacher is frozen (no gradients).\n","    \"\"\"\n","    model.train()\n","    if teacher is not None:\n","      teacher.eval()  # Teacher is always in eval mode\n","\n","    running_loss = 0.0\n","    running_hard_loss = 0.0\n","    running_soft_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in tqdm.tqdm(trainloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass for the trained model\n","        optimizer.zero_grad()\n","        logits = model(inputs)\n","\n","        # Compute loss\n","        if teacher is not None:\n","            # Get teacher predictions (no gradient needed)\n","            with torch.no_grad():\n","                teacher_logits = teacher(inputs)\n","            loss, hard_loss, soft_loss = criterion(\n","                logits, teacher_logits, labels\n","            )\n","            running_hard_loss += hard_loss.item()\n","            running_soft_loss += soft_loss.item()\n","        else:\n","          loss = criterion(logits, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item()\n","        _, predicted = logits.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    n = len(trainloader)\n","    return dict(\n","        total_loss=running_loss/n,\n","        hard_loss=running_hard_loss/n,\n","        soft_loss=running_soft_loss/n,\n","        train_acc=100.*correct/total\n","    )\n","\n","\n","def evaluate(model, testloader, device):\n","    \"\"\"Evaluate model on test set\"\"\"\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    return dict(test_acc=100. * correct / total)\n","\n","\n","def train_loop(\n","    model: nn.Module,\n","    criterion: nn.Module,\n","    checkpoint_name: str,\n","    model_name: str,\n","    experiment_name: str,\n","    train_data_loader: DataLoader,\n","    test_data_loader: DataLoader,\n","    teacher: Optional[nn.Module] = None,\n","    num_epochs: int = 5,\n","    learning_rate: float = 0.01,\n","    overwrite: bool = False,\n",") -> dict[str, list[float]]:\n","  \"\"\"Main training loop.\n","\n","  Args:\n","      model: Model to train.\n","      criterion: Loss function.\n","      checkpoint_name: Filename to save checkpoint.\n","      model_name: Display name for the model.\n","      experiment_name: Display name for the experiment.\n","      train_data_loader: Training loader.\n","      test_data_loader: Test loader.\n","      teacher: Optional teacher model.\n","      num_epochs: Number of epochs.\n","      learning_rate: Learning rate.\n","      overwrite: Whether to overwrite existing checkpoints.\n","\n","  Returns:\n","      dict[str, list[float]]: Training history.\n","  \"\"\"\n","  # Setup optimizer and scheduler\n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","  scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","      optimizer, T_max=num_epochs, eta_min=0.001\n","  )\n","\n","  print(experiment_name)\n","  print(\"=\" * 60)\n","  history = collections.defaultdict(list)\n","  checkpoint_file_name = f'{checkpoint_name}.pth'\n","\n","  # Check if checkpoint exists safely\n","  if os.path.exists(checkpoint_file_name) and not overwrite:\n","    raise ValueError(f'The checkpoint {checkpoint_file_name} already exists!')\n","\n","  best_test_acc = 0.0\n","  for epoch in range(num_epochs):\n","      train_start = time.time()\n","\n","      # Run training epoch\n","      metrics = train_epoch(\n","          model, teacher, train_data_loader, criterion, optimizer, device\n","      )\n","      train_time = time.time() - train_start\n","\n","      # Evaluation\n","      eval_start = time.time()\n","      metrics.update(evaluate(model, test_data_loader, device))\n","      eval_time = time.time() - eval_start\n","\n","      # Update learning rate\n","      scheduler.step()\n","\n","      # Store metrics\n","      for k, v in metrics.items():\n","          history[k].append(v)\n","\n","      train_acc = metrics.pop('train_acc')\n","      test_acc = metrics.pop('test_acc')\n","\n","      # Log progress\n","      print(\n","          f\"Epoch {epoch+1:2d}/{num_epochs}\",\n","          ' |'.join(f\" {k}: {v:.4f}\" for k, v in metrics.items()),\n","          f\"| Train Acc {train_acc:.2f}% | Test Acc {test_acc:.2f}%\"\n","          f\"| Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n","      )\n","\n","      # Save best model\n","      if test_acc > best_test_acc:\n","          best_test_acc = test_acc\n","          torch.save(model.state_dict(), checkpoint_file_name)\n","\n","  print(f\"\\nâœ… {model_name} final test accuracy: {best_test_acc:.2f}%\")\n","  return history"]},{"cell_type":"markdown","metadata":{"id":"_Fi3_7I1U2Kj"},"source":["## 6. Train the Teacher Model\n","\n","First, we need a well-trained teacher. In practice, you might use a pre-trained model, but we'll train one from scratch for educational purposes.\n","\n","**Note**: To save time, we'll provide the checkpoint, but you can check the training code and train your own teacher later."]},{"cell_type":"markdown","metadata":{"id":"b5ScyNaA0uWV"},"source":["### 6.1. Download the Teacher Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzQd6zjx2QEk"},"outputs":[],"source":["!gdown --id 1Ko41G-TVerBr1tY0cSr4m1h1s9PvHRXw"]},{"cell_type":"markdown","metadata":{"id":"xXEguB550pey"},"source":["### 6.2. Train the Teacher Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBLrBq-qU2Kj"},"outputs":[],"source":["# # Training configuration\n","# TEACHER_EPOCHS = 15  # Increase for better teacher (50+ recommended)\n","# LEARNING_RATE = 0.01\n","\n","# teacher = get_teacher_model(weights='IMAGENET1K_V1').to(device)\n","# teacher_history = train_loop(\n","#     model=teacher,\n","#     criterion=nn.CrossEntropyLoss(),\n","#     train_data_loader=trainloader,\n","#     test_data_loader=testloader,\n","#     num_epochs=TEACHER_EPOCHS,\n","#     learning_rate=LEARNING_RATE,\n","#     checkpoint_name=\"teacher_resnet18.pth\",\n","#     model_name=\"Teacher\",\n","#     experiment_name=\"Teacher Model (ResNet-18)\",\n","#     overwrite=False,\n","\n","# )"]},{"cell_type":"markdown","metadata":{"id":"DF7uzQ5WU2Kj"},"source":["## 7. Baseline: Train Student WITHOUT Distillation\n","\n","Let's start by training the student network with only hard labels. This will allow us to appreciate the value of distillation."]},{"cell_type":"code","source":["#@title Training Hypers\n","STUDENT_EPOCHS = 5\n","LR = 0.01"],"metadata":{"id":"7vu0o0ADNvWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ansQrnAHU2Kj"},"outputs":[],"source":["# Train student from scratch (no teacher)\n","teacher_dim = teacher.fc.in_features\n","student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","baseline_history = train_loop(\n","    model=student_baseline,\n","    criterion=nn.CrossEntropyLoss(),\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"baseline_student_v1\",\n","    model_name=\"Baseline Student\",\n","    experiment_name=\"Training Student Baseline (NO distillation)\",\n","    overwrite=False,\n","    # overwrite=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"-smlZ6nEU2Kj"},"source":["## 8. The Core: Knowledge Distillation Loss\n","\n","### The Distillation Loss Function\n","The key innovation is combining two losses:\n","$$L_{total} = \\alpha \\cdot L_{hard} + (1 - \\alpha) \\cdot L_{soft}$$\n","Where:\n","- $L_{hard}$ = Cross-entropy with true labels (standard supervised loss)\n","- $L_{soft}$ = KL divergence between teacher and student soft predictions\n","- $\\alpha$ = Weight balancing the two losses (typically 0.1-0.5)"]},{"cell_type":"markdown","metadata":{"id":"7G2wB-ocL6uH"},"source":["### Activity 2. Implement the `DistillationLoss` class in accordance with the previously provided description.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfucajXBMgCT"},"outputs":[],"source":["class DistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss.\n","\n","    Combines:\n","    1. Hard label loss: classification cross entropy.\n","    2. Soft label loss: KL divergence between the student and teacher logits.\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, alpha: float):\n","        super().__init__()\n","        # TODO\n","\n","    def forward(\n","          self,\n","          student_logits: torch.Tensor,\n","          teacher_logits: torch.Tensor,\n","          labels: torch.Tensor\n","        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        # TODO\n","        return total_loss, hard_loss, soft_loss"]},{"cell_type":"markdown","metadata":{"id":"gUJR7uwBMa7n"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaju1hJFU2Kj"},"outputs":[],"source":["class DistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss.\n","\n","    Combines:\n","    1. Hard label loss: classification cross entropy.\n","    2. Soft label loss: KL divergence between the student and teacher logits.\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, alpha: float):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(\n","          self,\n","          student_logits: torch.Tensor,\n","          teacher_logits: torch.Tensor,\n","          labels: torch.Tensor\n","        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        # Hard label loss (standard cross-entropy)\n","        hard_loss = self.ce_loss(student_logits, labels)\n","\n","        # Soft label loss (KL divergence)\n","        # Student: log-softmax\n","        student_logits = F.log_softmax(student_logits, dim=1)\n","        # Teacher: softmax (target distribution)\n","        teacher_probs = F.softmax(teacher_logits, dim=1)\n","\n","        # KL divergence * T^2 (to match gradient magnitude)\n","        soft_loss = self.kl_loss(student_logits, teacher_probs)\n","\n","        # Combined loss\n","        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n","\n","        return total_loss, hard_loss, soft_loss"]},{"cell_type":"markdown","metadata":{"id":"IDlTPBQuU2Kj"},"source":["## 9. Train Student with Distillation"]},{"cell_type":"markdown","metadata":{"id":"JkpsqpVQOpJJ"},"source":["### Training with distillation (Using big network as teacher)"]},{"cell_type":"markdown","source":["#### Activity 3. Implement the training code that uses the distillation loss defined above to distill knowledge from a large pretrained teacher model to a smaller student model."],"metadata":{"id":"YlEB46UlXf-W"}},{"cell_type":"code","source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","# TODO\n","\n","# Load student\n","# TODO\n","\n","\n","# Define training loss\n","# TODO\n","\n","# Start the model training.\n","# TODO"],"metadata":{"id":"fyqHDc-wYb0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"x5eXTxpfYXaf"}},{"cell_type":"code","source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","# Load student\n","teacher_dim = teacher.fc.in_features\n","student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Define training loss\n","distill_criterion = DistillationLoss(alpha=ALPHA)\n","\n","# Start the model training.\n","distilled_history = train_loop(\n","    model=student_distilled,\n","    teacher=teacher,\n","    criterion=distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"student_distilled_v1\",\n","    model_name=\"Distilled Student\",\n","    experiment_name=\"Training Student with Knowledge Distillation\",\n","    overwrite=False,\n","    # overwrite=True,\n",")"],"metadata":{"id":"QefQ8XKeEbyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJ2EjM3zU2Kj"},"source":["## 10. Results Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnHzjg9PU2Kj"},"outputs":[],"source":["# Final comparison\n","teacher_acc = evaluate(teacher, testloader, device)['test_acc']\n","distilled_acc = evaluate(student_distilled, testloader, device)['test_acc']\n","baseline_acc = evaluate(student_baseline, testloader, device)['test_acc']\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ðŸ“Š FINAL RESULTS\")\n","print(\"=\"*60)\n","print(f\"{'Model':<25} {'Parameters':<15} {'Test Accuracy':<15}\")\n","print(\"-\"*60)\n","print(f\"{'Teacher (ResNet-18)':<25} {teacher_params:>12,} {teacher_acc:>12.2f}%\")\n","print(f\"{'Student (Baseline)':<25} {student_params:>12,} {baseline_acc:>12.2f}%\")\n","print(f\"{'Student (Distilled)':<25} {student_params:>12,} {distilled_acc:>12.2f}%\")\n","print(\"-\"*60)\n","print(f\"\\nðŸ“ˆ Distillation improvement: +{distilled_acc - baseline_acc:.2f}%\")\n","print(f\"ðŸ—œï¸  Compression ratio: {teacher_params / student_params:.1f}x fewer parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2UF_dpoU2Kj","cellView":"form"},"outputs":[],"source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","xs = np.arange(1, STUDENT_EPOCHS + 1)\n","# Test accuracy comparison\n","axes[0].plot(xs, baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].axhline(y=teacher_acc, color='r', linestyle=':', label=f'Teacher ({teacher_acc:.1f}%)')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Loss breakdown for distilled student\n","axes[1].plot(xs, distilled_history['hard_loss'], label='Hard Loss (CE)', alpha=0.8)\n","axes[1].plot(xs, distilled_history['soft_loss'], label='Soft Loss (KL)', alpha=0.8)\n","axes[1].plot(xs, distilled_history['total_loss'], label='Total Loss', linewidth=2)\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"q69bvOz2U2Kj"},"source":["## 11. Inference Speed Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnZ4Ix0_nBrL"},"outputs":[],"source":["def benchmark_inference(\n","    model: nn.Module,\n","    input_size: tuple = (1, 3, 32, 32),\n","    num_iterations: int = 1000\n",") -> float:\n","    \"\"\"Benchmark inference speed.\n","\n","    Args:\n","        model (nn.Module): Model to benchmark.\n","        input_size (tuple): Input tensor shape.\n","        num_iterations (int): Number of iterations to average.\n","\n","    Returns:\n","        float: Average time per inference in milliseconds.\n","    \"\"\"\n","    model.eval()\n","    dummy_input = torch.randn(input_size).to(device)\n","\n","    # Warmup\n","    for _ in range(100):\n","        with torch.no_grad():\n","            _ = model(dummy_input)\n","\n","    # Benchmark\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    start = time.time()\n","    for _ in range(num_iterations):\n","        with torch.no_grad():\n","            _ = model(dummy_input)\n","\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    elapsed = time.time() - start\n","    return elapsed / num_iterations * 1000  # ms per inference\n","\n","teacher_time = benchmark_inference(teacher)\n","student_time = benchmark_inference(student_distilled)\n","\n","print(\"\\nâš¡ Inference Speed (single sample)\")\n","print(\"-\" * 40)\n","print(f\"Teacher (ResNet-18): {teacher_time:.3f} ms\")\n","print(f\"Student (Distilled): {student_time:.3f} ms\")\n","print(f\"Speedup: {teacher_time / student_time:.2f}x faster\")"]},{"cell_type":"markdown","metadata":{"id":"Ldi4RQTnnBrL"},"source":["## 12. Temperature Scaling\n","We \"soften\" the probability distributions using temperature $T$:\n","$$p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n","- $T = 1$: Normal softmax\n","- $T > 1$: Softer distribution (reveals more information about class relationships)\n","- Typically $T \\in [3, 20]$\n","\n","Note that the temperature is applied to both the teacher and the student outputs.\n","As the temperature T increases, the gradient of the loss gets rescaled by a factor of (1/T)^2. This has been derived in the original Hinton paper. As decreasing gradient value can affect optimization dynamics, we need to upscale the gradients by a factor of T^2. The easiest way to do it is just to multiply the loss by T^2."]},{"cell_type":"markdown","metadata":{"id":"QC2D5dwGnBrL"},"source":["### ðŸ” Let's Visualize Temperature Effects\n","\n","Understanding how temperature affects the probability distribution is crucial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_C9j-conBrL"},"outputs":[],"source":["# Demonstrate temperature effect\n","def visualize_temperature_effect(logits: torch.Tensor, temperatures: list = [1, 2, 4, 8, 16]) -> None:\n","    \"\"\"Visualize how temperature affects the softmax distribution.\n","\n","    Args:\n","        logits (torch.Tensor): Output logits from a model (1D tensor).\n","        temperatures (list): List of temperatures to visualize.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n","\n","    for ax, T in zip(axes, temperatures):\n","        probs = F.softmax(logits / T, dim=0).numpy()\n","        ax.bar(range(len(probs)), probs, color='steelblue')\n","        ax.set_title(f'T = {T}')\n","        ax.set_xlabel('Class')\n","        ax.set_ylabel('Probability')\n","        ax.set_ylim(0, 1)\n","        ax.set_xticks(range(10))\n","\n","    plt.suptitle('Effect of Temperature on Softmax Distribution', fontsize=14)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Example logits (model is quite confident about class 3)\n","example_logits = torch.tensor([1.0, 2.0, 1.5, 8.0, 0.5, 2.5, 1.0, 0.8, 1.2, 1.8])\n","visualize_temperature_effect(example_logits)\n","\n","print(\"\\nðŸ“Š Observation:\")\n","print(\"- T=1: Sharp distribution (almost one-hot)\")\n","print(\"- T>1: Softer distribution revealing class relationships\")\n","print(\"- Higher T = more 'dark knowledge' transferred\")"]},{"cell_type":"markdown","metadata":{"id":"6U1-buA-nBrL"},"source":["### Activity 4. Implement the `TemperedDistillationLoss` class.\n","It should be similar to `DistillationLoss` but with the temperature scaling applied to the teacher and student probabilities, and rescaled loss.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTeZL7pwnBrL"},"outputs":[],"source":["class TemperedDistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss with Temperature Scaling.\n","\n","    Combines:\n","    1. Hard label loss: CrossEntropy(student_output, true_labels)\n","    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, temperature: float = 4.0, alpha: float = 0.3) -> None:\n","        \"\"\"Initialize TemperedDistillationLoss.\n","\n","        Args:\n","            temperature (float): Temperature for softening the distribution.\n","            alpha (float): Weight for hard loss (1-alpha for soft loss).\n","        \"\"\"\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(\n","        self,\n","        student_logits: torch.Tensor,\n","        teacher_logits: torch.Tensor,\n","        labels: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"Compute the tempered distillation loss.\n","\n","        Args:\n","            student_logits: Logits from student model.\n","            teacher_logits: Logits from teacher model.\n","            labels: True labels.\n","\n","        Returns:\n","            tuple: (total_loss, hard_loss, soft_loss)\n","        \"\"\"\n","        # Hard label loss (standard cross-entropy)\n","        hard_loss = self.ce_loss(student_logits, labels)\n","\n","        # TODO: student_soft and teacher_soft with temperature scaling.\n","        student_soft = None\n","        teacher_soft = None\n","\n","        # KL divergence * T^2 (to match gradient magnitude)\n","        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n","\n","        # Combined loss\n","        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n","\n","        return total_loss, hard_loss, soft_loss"]},{"cell_type":"markdown","metadata":{"id":"_2xr9dKsnBrL"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ0j8wK7nBrL"},"outputs":[],"source":["class TemperedDistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss with Temperature Scaling.\n","\n","    Combines:\n","    1. Hard label loss: CrossEntropy(student_output, true_labels)\n","    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, temperature: float = 4.0, alpha: float = 0.3) -> None:\n","        \"\"\"Initialize TemperedDistillationLoss.\n","\n","        Args:\n","            temperature (float): Temperature for softening the distribution.\n","            alpha (float): Weight for hard loss (1-alpha for soft loss).\n","        \"\"\"\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(\n","        self,\n","        student_logits: torch.Tensor,\n","        teacher_logits: torch.Tensor,\n","        labels: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"Compute the tempered distillation loss.\n","\n","        Args:\n","            student_logits: Logits from student model.\n","            teacher_logits: Logits from teacher model.\n","            labels: True labels.\n","\n","        Returns:\n","            tuple: (total_loss, hard_loss, soft_loss)\n","        \"\"\"\n","        # Hard label loss (standard cross-entropy)\n","        hard_loss = self.ce_loss(student_logits, labels)\n","\n","        # Soft label loss (KL divergence with temperature)\n","        # Student: log-softmax with temperature\n","        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n","        # Teacher: softmax with temperature (target distribution)\n","        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n","\n","        # KL divergence * T^2 (to match gradient magnitude)\n","        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n","\n","        # Combined loss\n","        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n","\n","        return total_loss, hard_loss, soft_loss"]},{"cell_type":"markdown","source":["### Training with temperature-scaled distillation (Using big network as teacher)"],"metadata":{"id":"W2ee5vCuqHK1"}},{"cell_type":"code","source":["# Distillation configuration\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","TEMPERATURE = 4.0  # Try values in [1, 20]\n","LR = 0.01\n","\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","teacher_dim = teacher.fc.in_features\n","student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","tempered_distill_criterion = TemperedDistillationLoss(\n","    temperature=TEMPERATURE,\n","    alpha=ALPHA\n",")\n","\n","tempered_distillation_history = train_loop(\n","    model=student,\n","    teacher=teacher,\n","    criterion=tempered_distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"student_distilled_tempered_v1\",\n","    model_name=\"Tempered-distillation Student\",\n","    experiment_name=\"Training Student with Tempered Knowledge Distillation\",\n","    overwrite=False,\n",")"],"metadata":{"id":"A0JJ57drqHsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Test accuracy comparison (axes[0] remains as is, it already has distinct labels and lines)\n","xs = np.arange(1, len(baseline_history['test_acc']) + 1)\n","axes[0].plot(xs, baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].plot(xs, tempered_distillation_history['test_acc'], label='Student (Tempered Distillation)', linewidth=2)\n","axes[0].axhline(y=teacher_acc, color='r', linestyle=':', label=f'Teacher ({teacher_acc:.1f}%)')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Loss breakdown for distilled student (axes[1])\n","# Define consistent colors for the loss components\n","hard_loss_color = 'tab:blue'\n","soft_loss_color = 'tab:orange'\n","total_loss_color = 'tab:green'\n","\n","# Plot original distillation losses with solid lines\n","axes[1].plot(xs, distilled_history['hard_loss'], label='Hard Loss (Distilled)', alpha=0.8, color=hard_loss_color)\n","axes[1].plot(xs, distilled_history['soft_loss'], label='Soft Loss (Distilled)', alpha=0.8, color=soft_loss_color)\n","axes[1].plot(xs, distilled_history['total_loss'], label='Total Loss (Distilled)', linewidth=2, color=total_loss_color)\n","\n","# Plot tempered distillation losses with dashed lines and the same colors\n","axes[1].plot(xs, tempered_distillation_history['hard_loss'], alpha=0.8, color=hard_loss_color, linestyle='--')\n","axes[1].plot(xs, tempered_distillation_history['soft_loss'], alpha=0.8, color=soft_loss_color, linestyle='--')\n","axes[1].plot(xs, tempered_distillation_history['total_loss'], linewidth=2, color=total_loss_color, linestyle='--')\n","axes[1].plot([], [], linewidth=2, linestyle='--', c='k', label='Tempered')\n","\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rjsJgnAyqRdT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 13. Distillation Aids Generalization\n","(particularly in a low data context)"],"metadata":{"id":"vo30wHxYvdru"}},{"cell_type":"code","source":["# Grayscale + Noise (old camera simulation)\n","class AddGaussianNoise:\n","    \"\"\"Add Gaussian noise to a tensor.\"\"\"\n","    def __init__(self, mean: float = 0., std: float = 0.1) -> None:\n","        \"\"\"Initialize the transform.\n","\n","        Args:\n","            mean (float): Mean of the noise.\n","            std (float): Standard deviation of the noise.\n","        \"\"\"\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Apply noise to the tensor.\n","\n","        Args:\n","            tensor (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Noisy tensor.\n","        \"\"\"\n","        noise = torch.randn_like(tensor) * self.std + self.mean\n","        return torch.clamp(tensor + noise, 0., 1.)\n","\n","transform_train_grayscale_noisy = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(mean=0., std=0.02),\n","    transforms.Normalize(cifar_mean, cifar_std),\n","])\n","\n","transform_test_grayscale_noisy = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(mean=0., std=0.02),\n","    transforms.Normalize(cifar_mean, cifar_std),\n","])\n","\n","# Download and load datasets\n","augmented_cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train_grayscale_noisy)\n","augmented_cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test_grayscale_noisy)\n","\n","augmented_cifar10_trainloader = DataLoader(augmented_cifar10_train, batch_size=128, shuffle=True, num_workers=0)\n","augmented_cifar10_testloader = DataLoader(augmented_cifar10_test, batch_size=128, shuffle=False, num_workers=0)"],"metadata":{"id":"FWzuSnVRvqIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 5. Plot some instances\n","Similar to the original CIFAR-10 dataset, plot a few sample instances to assess the impact of the applied transformation."],"metadata":{"id":"5N4kYrCiwDv8"}},{"cell_type":"code","source":["# Get some random training images\n","dataiter_v2 = iter(augmented_cifar10_trainloader)\n","images, labels = next(dataiter_v2)\n","\n","# Show images\n","# TODO"],"metadata":{"id":"CQGqceqRwAbm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"teUFxYwHwLcI"}},{"cell_type":"code","source":["# Get some random training images\n","dataiter_v2 = iter(augmented_cifar10_trainloader)\n","images, labels = next(dataiter_v2)\n","\n","# Show images\n","fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n","for i in range(8):\n","    imshow(axes[i], images[i])\n","    axes[i].set_title(classes[labels[i]])\n","    axes[i].axis('off')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Oh1TyYjRwAeP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 6. Train the student without distillation on the new dataset."],"metadata":{"id":"LGBnuM8Pw8j6"}},{"cell_type":"code","source":["# TODO"],"metadata":{"id":"O1BT6BgzwAgw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"oxrjLMAfw-3t"}},{"cell_type":"code","source":["# Train student from scratch (no teacher)\n","STUDENT_EPOCHS = 5\n","LR = 0.01\n","\n","teacher_dim = teacher.fc.in_features\n","aug_cifar10_student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","aug_cifar10_student_baseline_history = train_loop(\n","    model=aug_cifar10_student_baseline,\n","    teacher=None,\n","    criterion=nn.CrossEntropyLoss(),\n","    train_data_loader=augmented_cifar10_trainloader,\n","    test_data_loader=augmented_cifar10_testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"augmented_cifar_student_no_distillation\",\n","    model_name=\"Augmented Cifar-10 Baseline\",\n","    experiment_name=\"Training Student Baseline (NO distillation)\",\n","    overwrite=False,\n",")"],"metadata":{"id":"_gNImKg3wAjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 7. Train the student network on the new dataset using the TemperedDistillationLoss class, and compare the results against those obtained from training the student model independently."],"metadata":{"id":"-WsPJLzMxGVr"}},{"cell_type":"code","source":["# TODO"],"metadata":{"id":"fTwpjrgRxKrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"kwGAzz5BxInx"}},{"cell_type":"code","source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","teacher_dim = teacher.fc.in_features\n","aug_cifar10_student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n","\n","aug_cifar10_student_distilled_history = train_loop(\n","    model=aug_cifar10_student_distilled,\n","    teacher=teacher,\n","    criterion=distill_criterion,\n","    train_data_loader=augmented_cifar10_trainloader,\n","    test_data_loader=augmented_cifar10_testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"augmented_cifar_distilled_student\",\n","    model_name=\"Augmented Cifar-10 Distilled Student\",\n","    experiment_name=\"Training Distilled student on augmented Cifar-10\",\n","    overwrite=False,\n",")"],"metadata":{"id":"JYtYKifWxM0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","xs = np.arange(1, STUDENT_EPOCHS + 1)\n","# Test accuracy comparison\n","axes[0].plot(xs, aug_cifar10_student_baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, aug_cifar10_student_distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","hard_loss_col = \"tab:blue\"\n","# Loss breakdown for distilled student\n","axes[1].plot(xs, aug_cifar10_student_distilled_history['hard_loss'], label='Hard Loss (CE)', alpha=0.8)\n","axes[1].plot(xs, aug_cifar10_student_distilled_history['soft_loss'], label='Soft Loss (KL)', alpha=0.8)\n","axes[1].plot(xs, aug_cifar10_student_distilled_history['total_loss'], label='Total Loss', linewidth=2)\n","axes[1].plot(\n","    xs, aug_cifar10_student_baseline_history['total_loss'],\n","    label='Hard Loss (Baseline)', alpha=0.8, c=hard_loss_col, linestyle='--'\n",")\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"u0d0G37451SC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnA2AzFtnBrL"},"source":["## 13. Born Again Networks (Use the previous trained model (Student) as a Teacher)."]},{"cell_type":"markdown","source":["You train a model once (the Teacher). Then, you take a fresh version of the exact same architecture (the Student) and train it to not only hit the correct labels but also to mimic the \"confidence\" of the Teacher. Surprisingly, the Student almost always ends up smarter than the Teacher. This concept was introduced by [Furlanello et al](https://arxiv.org/pdf/1805.04770)."],"metadata":{"id":"-yrdYkIFdpee"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQh0rizznBrM"},"outputs":[],"source":["# Distillation configuration\n","STUDENT_EPOCHS = 30\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher_dim = teacher.fc.in_features\n","teacher_st_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","teacher_st_baseline.load_state_dict(torch.load('student_baseline.pth'))\n","teacher_st_baseline.eval()\n","\n","student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n","\n","ban_history = train_loop(\n","    model=student,\n","    teacher=teacher,\n","    criterion=tempered_distill_criterion,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"ban_v1\",\n","    model_name=\"Born-Again Network\",\n","    experiment_name=\"Training Student Born-Again Network\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"-BKAMBSPU2Kj"},"source":["## 14. ðŸ§ª Experiment: Hyperparameter Sensitivity\n","\n","Try different values of **Temperature** and **Alpha** to see their effects!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-uv75XeU2Kj"},"outputs":[],"source":["def quick_distillation_experiment(temperature, alpha, epochs=15):\n","    \"\"\"Quick experiment with different hyperparameters\"\"\"\n","    student = StudentCNN().to(device)\n","    criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n","    optimizer = optim.Adam(student.parameters(), lr=0.001)\n","\n","    for epoch in range(epochs):\n","        train_with_distillation(student, teacher, trainloader, criterion, optimizer, device)\n","\n","    return evaluate(student, testloader, device)\n","\n","# Uncomment to run experiments (takes a few minutes)\n","# print(\"Running hyperparameter experiments...\")\n","\n","# temperatures = [1, 2, 4, 8, 16]\n","# alphas = [0.1, 0.3, 0.5, 0.7]\n","\n","# results = {}\n","# for T in temperatures:\n","#     for a in alphas:\n","#         acc = quick_distillation_experiment(T, a)\n","#         results[(T, a)] = acc\n","#         print(f\"T={T}, Î±={a}: {acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"nbJ2ZtW2U2Kk"},"source":["## 15. Types of Knowledge Distillation\n","\n","What we implemented is **Response-based Distillation**. There are other types:\n","\n","| Type | What's Transferred | Example |\n","|------|-------------------|--------|\n","| **Response-based** | Final output logits | What we did! |\n","| **Feature-based** | Intermediate representations | FitNets, Attention Transfer |\n","| **Relation-based** | Relationships between samples | Contrastive distillation |\n","\n","### Bonus: Feature-based Distillation (FitNets)\n","\n","The idea is to also match intermediate feature maps, not just outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMPhV21tU2Kk"},"outputs":[],"source":["class FeatureDistillationLoss(nn.Module):\n","    \"\"\"\n","    Feature-based distillation: match intermediate representations.\n","    Requires a projection layer if dimensions don't match.\n","    \"\"\"\n","    def __init__(self, device, student_dim, teacher_dim, temperature=4.0, alpha=0.3, beta=0.5):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.beta = beta  # Weight for feature loss\n","\n","        # Projection layer if dimensions don't match\n","        self.projector = nn.Linear(student_dim, teacher_dim) if student_dim != teacher_dim else nn.Identity()\n","        self.projector = self.projector.to(device)\n","\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","        self.mse_loss = nn.MSELoss()\n","\n","    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n","        # Standard distillation losses\n","        hard_loss = self.ce_loss(student_logits, labels)\n","        soft_loss = self.kl_loss(\n","            F.log_softmax(student_logits / self.temperature, dim=1),\n","            F.softmax(teacher_logits / self.temperature, dim=1)\n","        ) * (self.temperature ** 2)\n","\n","        # Feature matching loss\n","        student_proj = self.projector(student_features)\n","        feature_loss = 1 - F.cosine_similarity(student_proj, teacher_features).mean()\n","\n","        # Combined loss\n","        total = self.alpha * hard_loss + (1 - self.alpha - self.beta) * soft_loss + self.beta * 10 * feature_loss\n","\n","        return total, hard_loss, soft_loss, feature_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq63hq70etll"},"outputs":[],"source":["def train_with_feature_distillation(student, teacher, trainloader, criterion, optimizer, device):\n","    \"\"\"\n","    Train student using knowledge distillation from teacher.\n","    Teacher is frozen (no gradients).\n","    \"\"\"\n","    student.train()\n","    teacher.eval()  # Teacher is always in eval mode\n","\n","    running_loss = 0.0\n","    running_hard_loss = 0.0\n","    running_soft_loss = 0.0\n","    running_feature_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    tqdm_bar = tqdm.auto.tqdm\n","    for inputs, labels in tqdm_bar(trainloader, desc=\"Distilling\", leave=False):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Get teacher predictions (no gradient needed)\n","        with torch.no_grad():\n","            teacher_logits, teacher_features = teacher(inputs, output_features=True)\n","\n","        # Forward pass for student\n","        optimizer.zero_grad()\n","        student_logits, student_features = student(inputs, output_features=True)\n","\n","        # Compute distillation loss\n","        loss, hard_loss, soft_loss, feature_loss = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item()\n","        running_hard_loss += hard_loss.item()\n","        running_soft_loss += soft_loss.item()\n","        running_feature_loss += feature_loss.item()\n","        _, predicted = student_logits.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    n = len(trainloader)\n","    return running_loss/n, running_hard_loss/n, running_soft_loss/n, running_feature_loss/n, 100.*correct/total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqGC_W1-aGoa"},"outputs":[],"source":["# Distillation configuration\n","STUDENT_EPOCHS = 30\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","BETA = 0.5\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","# Create fresh student\n","teacher_dim = teacher.fc.in_features\n","student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","student_dim = student_distilled.fc2[1].in_features\n","teacher_dim = teacher.fc.in_features\n","distill_criterion = FeatureDistillationLoss(device, student_dim, teacher_dim, temperature=TEMPERATURE, alpha=ALPHA, beta=BETA)\n","parameters = list(student_distilled.parameters()) + list(distill_criterion.projector.parameters())\n","optimizer = optim.Adam(parameters, lr=LR)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001)\n","\n","print(f\"Training Student with Knowledge Distillation\")\n","print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}, Beta: {BETA}\")\n","print(\"=\" * 60)\n","\n","distilled_history = {'total_loss': [], 'hard_loss': [], 'soft_loss': [], 'feature_loss': [],'train_acc': [], 'test_acc': []}\n","\n","for epoch in range(STUDENT_EPOCHS):\n","    total_loss, hard_loss, soft_loss, feature_loss, train_acc = train_with_feature_distillation(\n","        student_distilled, teacher, trainloader, distill_criterion, optimizer, device\n","    )\n","    test_acc = evaluate(student_distilled, testloader, device)\n","    scheduler.step()\n","\n","    distilled_history['total_loss'].append(total_loss)\n","    distilled_history['hard_loss'].append(hard_loss)\n","    distilled_history['soft_loss'].append(soft_loss)\n","    distilled_history['feature_loss'].append(feature_loss)\n","    distilled_history['train_acc'].append(train_acc)\n","    distilled_history['test_acc'].append(test_acc)\n","\n","    print(f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS} | Loss: {total_loss:.4f} (H:{hard_loss:.3f} S:{soft_loss:.3f} F:{feature_loss:.3f}) | \"\n","          f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n","\n","print(f\"\\nâœ… Distilled Student final test accuracy: {test_acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"VtuOdh5rU2Kk"},"source":["## 16. Key Takeaways\n","\n","### âœ… What We Learned\n","\n","1. **Knowledge Distillation** transfers \"dark knowledge\" from a large teacher to a small student\n","\n","2. **Soft labels** contain richer information than hard labels (class relationships)\n","\n","3. **Temperature** controls how soft the probability distribution is:\n","   - Higher T = more information transfer, but potentially noisier\n","   - Typical values: 2-20\n","\n","4. **Alpha** balances hard and soft losses:\n","   - Lower Î± = more emphasis on mimicking teacher\n","   - Higher Î± = more emphasis on ground truth\n","\n","5. **Results**: Distilled students typically outperform students trained from scratch by 1-5%\n","\n","### ðŸš€ Extensions to Explore\n","\n","- **Self-distillation**: Use the same architecture for teacher and student\n","- **Online distillation**: Train teacher and student simultaneously\n","- **Multi-teacher distillation**: Ensemble of teachers\n","- **Task-specific distillation**: For NLP, use DistilBERT approach\n","\n","### ðŸ“š References\n","\n","1. Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n","2. Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2015)\n","3. Gou et al., \"Knowledge Distillation: A Survey\" (2021)"]},{"cell_type":"markdown","metadata":{"id":"BMl6-H3jU2Kk"},"source":["## 15. ðŸ’ª Exercise for You!\n","\n","Try these modifications and see what happens:\n","\n","1. **Change the student architecture**: Make it deeper or wider\n","2. **Try different temperatures**: Plot accuracy vs temperature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jbrT3-6U2Kk"},"outputs":[],"source":["# Your experiments here!\n","#\n","# Example: Try a deeper student\n","# class DeeperStudent(nn.Module):\n","#     def __init__(self):\n","#         super().__init__()\n","#         # Your architecture here\n","#         pass"]},{"cell_type":"markdown","source":["Implement computing gradient norms and compare the student network with and without distillation, and with different temperatures during distillation"],"metadata":{"id":"5Xz3La9EEKMt"}},{"cell_type":"code","source":["grad_norm = np.sqrt(sum([torch.norm(p.grad)**2 for p in model.parameters()]))"],"metadata":{"id":"8L1_IAMFERMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Vdf1QdWEJSw"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true,"collapsed_sections":["zWsLk8bmJ1wL","gUJR7uwBMa7n","x5eXTxpfYXaf","_2xr9dKsnBrL","teUFxYwHwLcI","oxrjLMAfw-3t","kwGAzz5BxInx"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}